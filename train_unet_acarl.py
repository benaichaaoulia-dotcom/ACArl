"""
Confidence-Weighted U-Net Training for ACArL-enhanced WeakMedSAM

This script trains a U-Net using confidence-weighted loss on pseudo-labels
generated by the ACArL module.
"""

import sys
sys.path.append(".")
import torch.optim as optim
from torch.utils.data import DataLoader
import torch
import torch.nn.functional as F
import argparse
import os
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
from utils.metrics import dice
from utils.pytuils import AverageMeter
from unet.unet_model import UNet
from torch.nn.functional import cross_entropy, one_hot
from torch.amp import autocast, GradScaler
import importlib


def confidence_weighted_dice_loss(pred, target, confidence, smooth=1.0):
    """
    Compute confidence-weighted Dice loss.
    
    Args:
        pred: (B, C, H, W) predicted logits
        target: (B, C, H, W) ground truth segmentation
        confidence: (B, 1, H, W) confidence map [0, 1]
        smooth: smoothing factor
    
    Returns:
        loss: scalar confidence-weighted Dice loss
    """
    pred_prob = torch.sigmoid(pred)
    
    # Apply confidence weighting
    weighted_pred = pred_prob * confidence
    weighted_target = target * confidence
    
    # Compute Dice coefficient
    intersection = (weighted_pred * weighted_target).sum(dim=(2, 3))
    union = weighted_pred.sum(dim=(2, 3)) + weighted_target.sum(dim=(2, 3))
    
    dice_score = (2.0 * intersection + smooth) / (union + smooth)
    dice_loss = 1.0 - dice_score.mean()
    
    return dice_loss


def confidence_weighted_ce_loss(pred, target, confidence, class_weights=None):
    """
    Compute confidence-weighted cross-entropy loss.
    
    Args:
        pred: (B, C, H, W) predicted logits
        target: (B, H, W) target labels
        confidence: (B, 1, H, W) confidence map [0, 1]
        class_weights: optional class weights
    
    Returns:
        loss: scalar confidence-weighted cross-entropy loss
    """
    # Ensure target is long dtype for cross_entropy
    target = target.long()
    # Standard cross-entropy
    ce_loss = cross_entropy(pred, target, weight=class_weights, reduction='none')  # (B, H, W)
    
    # Apply confidence weighting
    confidence_squeezed = confidence.squeeze(1)  # (B, H, W)
    weighted_loss = ce_loss * confidence_squeezed
    
    # Normalize by total confidence to prevent trivial solutions
    total_confidence = confidence_squeezed.sum() + 1e-8
    weighted_loss = weighted_loss.sum() / total_confidence
    
    return weighted_loss


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, required=True)
    parser.add_argument("--data_module", type=str, required=True)
    parser.add_argument("--lab_path", type=str, required=True)
    parser.add_argument("--conf_path", type=str, default=None,
                        help="Path to confidence maps (optional)")
    parser.add_argument("--logdir", type=str, required=True)
    parser.add_argument("--batch_size", type=int, default=8)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--index", type=str, required=True)
    parser.add_argument("--max_epochs", type=int, default=30)
    parser.add_argument("--val_iters", type=int, default=100)
    parser.add_argument("--num_classes", type=int, default=2)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--gpus", type=str, default="0")
    parser.add_argument("--use_confidence", action="store_true",
                        help="Use confidence-weighted loss")
    parser.add_argument("--conf_threshold", type=float, default=0.3,
                        help="Minimum confidence threshold for training")
    parser.add_argument("--loss_type", type=str, default="ce", choices=["ce", "dice", "combined"],
                        help="Loss type: ce (cross-entropy), dice, or combined")
    args = parser.parse_args()
    print(args)

    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpus

    torch.manual_seed(args.seed)
    torch.cuda.manual_seed(args.seed)
    torch.backends.cudnn.deterministic = True

    model = torch.nn.DataParallel(
        UNet(n_channels=1, n_classes=args.num_classes, bilinear=True)
    ).cuda()

    data_module = importlib.import_module(f"{args.data_module}.dataset")
    
    # Load dataset with confidence maps if available
    if args.use_confidence and args.conf_path:
        train_dataset, val_dataset, _ = data_module.get_seg_dataset_with_conf(
            args.data_path, args.lab_path, args.conf_path
        )
    else:
        train_dataset, val_dataset, _ = data_module.get_seg_dataset(
            args.data_path, args.lab_path
        )

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        drop_last=True,
        pin_memory=True,
        num_workers=4,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        drop_last=False,
        pin_memory=True,
        num_workers=4,
    )

    args.max_iters = args.max_epochs * len(train_loader)

    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.1)
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer=optimizer,
        max_lr=args.lr,
        total_steps=args.max_iters,
    )
    scaler = GradScaler()

    writer = SummaryWriter(os.path.join(args.logdir, args.index))

    pbar = tqdm(range(1, args.max_iters + 1), ncols=100, desc="iter")

    train_loader_iter = iter(train_loader)
    for n_iter in pbar:
        optimizer.zero_grad()
        model.train()
        try:
            datapack = next(train_loader_iter)
        except:
            train_loader_iter = iter(train_loader)
            datapack = next(train_loader_iter)

        imgs = datapack["img"].cuda()
        segs = datapack["seg"].cuda()
        labs = datapack["lab"].cuda()
        
        # Get confidence maps if available
        if args.use_confidence and "conf" in datapack:
            confidence = datapack["conf"].cuda()
            # Apply confidence threshold
            confidence = torch.clamp(confidence - args.conf_threshold, min=0.0) / (1.0 - args.conf_threshold)
        else:
            confidence = torch.ones_like(imgs)

        with autocast(device_type='cuda'):
            x = model(imgs)
            
            # Compute loss based on type
            if args.loss_type == "ce":
                loss = confidence_weighted_ce_loss(
                    x, labs, confidence, 
                    class_weights=torch.tensor([10.0, 1.0]).cuda() if args.num_classes == 2 else None
                )
            elif args.loss_type == "dice":
                # Convert labs to one-hot for Dice loss
                target_one_hot = one_hot(labs.long(), args.num_classes).permute(0, 3, 1, 2).float()
                loss = confidence_weighted_dice_loss(x, target_one_hot, confidence)
            else:  # combined
                ce_loss = confidence_weighted_ce_loss(
                    x, labs, confidence,
                    class_weights=torch.tensor([10.0, 1.0]).cuda() if args.num_classes == 2 else None
                )
                target_one_hot = one_hot(labs.long(), args.num_classes).permute(0, 3, 1, 2).float()
                dice_loss = confidence_weighted_dice_loss(x, target_one_hot, confidence)
                loss = 0.5 * ce_loss + 0.5 * dice_loss

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        scheduler.step()

        pred = (
            one_hot(
                x.argmax(dim=1),
                args.num_classes,
            )
            .permute(0, 3, 1, 2)
            .float()
        )
        score, cnt = dice(pred[:, 1:], segs)
        score = sum(score) / len(score) if score else 0

        if n_iter % args.val_iters == 0:
            model.eval()
            val_score = AverageMeter()
            with torch.no_grad():
                for pack in val_loader:
                    imgs = pack["img"].cuda()
                    segs = pack["seg"].cuda()
                    with autocast(device_type='cuda'):
                        x = model(imgs)
                        pred = (
                            one_hot(
                                x.argmax(dim=1),
                                args.num_classes,
                            )
                            .permute(0, 3, 1, 2)
                            .float()
                        )

                    val_score.add(*dice(pred[:, 1:], segs))
            writer.add_scalar("unet val/val score", val_score.get(), n_iter)

            model.train()

        writer.add_scalar("unet train/train loss", loss.item(), n_iter)
        writer.add_scalar("unet train/train score", score, n_iter)
        writer.add_scalar("unet train/lr", optimizer.param_groups[0]["lr"], n_iter)
        
        if args.use_confidence and "conf" in datapack:
            writer.add_scalar("unet train/mean confidence", confidence.mean().item(), n_iter)

        pbar.set_postfix(
            {
                "tl": loss.item(),
                "ts": score,
                "lr": optimizer.param_groups[0]["lr"],
            }
        )

    torch.save(
        model.module.state_dict(),
        os.path.join(args.logdir, args.index, f"{args.index}.pth"),
    )


if __name__ == "__main__":
    main()
